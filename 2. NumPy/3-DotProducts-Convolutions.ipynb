{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55803d1-10ce-4df4-af03-ab3c9f7313f0",
   "metadata": {},
   "source": [
    "# Install the dependencies for this lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70458c2-9302-4128-9060-fa27b4e45416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install matplotlib and scipy\n",
    "!pip install matplotlib scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7d335-1e83-4d10-a015-2f432cdc1cea",
   "metadata": {},
   "source": [
    "## 1. Introduction and Recap:\r\n",
    "\r\n",
    "### Recap of Vectors and Matrices:\r\n",
    "Remember, vectors are one-dimensional arrays while matrices are two-dimensional. They are foundational in linear algebra, and in our context, crucial for understanding dot products and convolutions.\r\n",
    "\r\n",
    "### Importance of Dot Products and Convolutions in AI:\r\n",
    "Dot products and convolutions form the heart of operations in neural networks, especially convolutional neural networks (CNNs). Understanding them is key to grasping the inner workings of these networks.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fe50e-d45a-48de-8572-b781579f4c01",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dot Products:\n",
    "\n",
    "### Definition:\n",
    "The dot product of two vectors is a scalar value produced by multiplying corresponding entries of two vectors and summing up those products. \n",
    "\n",
    "### Calculation:\n",
    "Given two vectors:\n",
    "A = [a1, a2, ... , an]\n",
    "B = [b1, b2, ... , bn]\n",
    "\n",
    "The dot product is:\n",
    "a1*b1 + a2*b2 + ... + an*bn\n",
    "\n",
    "### Let's try it manually with two vectors:\n",
    "A = [2, 3]\n",
    "B = [4, 5]\n",
    "\n",
    "Dot product = 2*4 + 3*5 = 8 + 15 = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546189d-08ea-42c7-9096-a0bf4ed17967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Let's define our vectors\n",
    "A = np.array([2, 3])\n",
    "B = np.array([4, 5])\n",
    "\n",
    "# Compute the dot product using numpy\n",
    "dot_product = np.dot(A, B)\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa69293-1412-4431-add7-6b21b2bdf88d",
   "metadata": {},
   "source": [
    "---\n",
    "### Interpretation:\r\n",
    "\r\n",
    "As we can see, the dot product we computed manually matches the one we computed using numpy! The dot product gives us a scalar value, which can be interpreted as a measure of how much one vector \"goes in the direction\" of another vector. This concept is crucial when we dive deeper into neural networks and optimization techniques.\r\n",
    "\r\n",
    "---\r\n",
    "### Hands-on Exercise:\r\n",
    "\r\n",
    "1. Compute the dot product of vectors [1, 2, 3] and [4, 5, 6] manually.\r\n",
    "2. Validate your answer using numpy.\r\n",
    "3. What is the geometric interpretation of the dot product when the angle between the vectors is 90 degrees? Why?\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9b837-02e1-4fa0-8356-c1defdb7e702",
   "metadata": {},
   "source": [
    "---\n",
    "## Geometric Interpretation of the Dot Product:\n",
    "\n",
    "The dot product has a neat geometric interpretation. Recall the definition:\n",
    "\n",
    "For two vectors \\( A \\) and \\( B \\), the dot product is defined as:\n",
    "$$ A \\cdot B = |A| \\times |B| \\times \\cos(\\theta) $$\n",
    "\n",
    "Where:\n",
    "- A is the magnitude (or length) of vector `A`\n",
    "- B is the magnitude of vector `B`\n",
    "- θ is the angle between the vectors\n",
    "\n",
    "### Case: When the angle θ is 90 degrees:\n",
    "\n",
    "If two vectors are perpendicular to each other (i.e., the angle between them is 90 degrees or π/2 radians), then the cosine of 90 degrees is 0.\n",
    "\n",
    "Thus, the dot product becomes:\n",
    "$$ A \\cdot B = |A| \\times |B| \\times 0 = 0 $$\n",
    "\n",
    "This means that when two vectors are perpendicular, their dot product is 0. This is a crucial property in linear algebra and is referred to as the vectors being **orthogonal**.\n",
    "\n",
    "### Let's explore this with an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11990bee-3f51-4f7c-93c3-201966cde590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two vectors that are orthogonal in 2D space\n",
    "A = np.array([1, 0])\n",
    "B = np.array([0, 1])\n",
    "\n",
    "# Compute the dot product using numpy\n",
    "dot_product_orthogonal = np.dot(A, B)\n",
    "dot_product_orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0aa194-d61f-461c-8b1e-efb6a92033be",
   "metadata": {},
   "source": [
    "As we can see, the dot product of two orthogonal vectors is indeed 0.\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "It's always helpful to visualize these concepts. Let's plot the two vectors to see their orthogonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c31342-8e6d-4713-b7fd-c834e36eb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the two vectors\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.quiver(0, 0, A[0], A[1], angles='xy', scale_units='xy', scale=1, color='r', label=\"Vector A\")\n",
    "ax.quiver(0, 0, B[0], B[1], angles='xy', scale_units='xy', scale=1, color='b', label=\"Vector B\")\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.axvline(x=0, color='grey', lw=1)\n",
    "ax.axhline(y=0, color='grey', lw=1)\n",
    "ax.legend()\n",
    "ax.set_title(\"Orthogonal Vectors A and B\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad6958-0bf7-4b13-babc-cd694becc29a",
   "metadata": {},
   "source": [
    "---\n",
    "The red and blue arrows represent our vectors `A` and `B`, respectively. As observed, they are perpendicular to each other.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "When the angle between two vectors is 90 degrees (making them perpendicular or orthogonal), their dot product is 0. This property has important implications in various areas of mathematics and computer science, especially in the context of neural networks and other machine learning algorithms where orthogonality often plays a key role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02811a3d-6312-495a-9a1e-921932bccb43",
   "metadata": {},
   "source": [
    "---\n",
    "## Hands-on Exercise:\n",
    "\n",
    "### 1. Compute the dot product of vectors [1, 2, 3] and [4, 5, 6] manually:\n",
    "\n",
    "Given two vectors:\n",
    "$$ [ C = [c_1, c_2, c_3] ] $$\n",
    "$$ [ D = [d_1, d_2, d_3] ] $$\n",
    "\n",
    "The dot product is:\n",
    "\n",
    "$$ [ C \\cdot D = c_1 \\times d_1 + c_2 \\times d_2 + c_3 \\times d_3 ] $$\n",
    "\n",
    "Using the given vectors:\n",
    "$$ [ C = [1, 2, 3]  ] $$\n",
    "$$ [ D = [4, 5, 6] ] $$\n",
    "\n",
    "The dot product becomes:\n",
    "$$ [ C \\cdot D = 1 \\times 4 + 2 \\times 5 + 3 \\times 6 = 4 + 10 + 18 = 32 ] $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Validate your answer using numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c04129-95be-4ced-817a-e584c654c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectors\n",
    "C = np.array([1, 2, 3])\n",
    "D = np.array([4, 5, 6])\n",
    "\n",
    "# Compute the dot product using numpy\n",
    "dot_product_CD = np.dot(C, D)\n",
    "dot_product_CD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bc265-a91c-48be-9ae0-ccf02908e5e3",
   "metadata": {},
   "source": [
    "---\n",
    "The dot product computed manually and using numpy matches! The result is indeed 32.\n",
    "\n",
    "### 3. What is the geometric interpretation of the dot product when the angle between the vectors is 90 degrees? Why?\n",
    "\n",
    "As previously discussed, when the angle between two vectors is 90 degrees, the vectors are considered orthogonal or perpendicular. Geometrically, this means they do not share any component in the direction of each other. In the context of the dot product, this is captured by the cosine of the angle between them. \n",
    "\n",
    "For an angle of 90 degrees (or \\( \\pi/2 \\) radians), the cosine value is 0. Hence, the dot product formula:\n",
    "$$ A \\cdot B = |A| \\times |B| \\times \\cos(\\theta) $$\n",
    "\n",
    "Becomes:\n",
    "$$ A \\cdot B = |A| \\times |B| \\times 0 = 0 $$\n",
    "\n",
    "Thus, when two vectors are orthogonal, their dot product is 0. This property reinforces the idea that the vectors don't have any shared directionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d66efa-b802-4fdd-9645-c3841438450f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Convolutions:\n",
    "\n",
    "### Introduction to Convolutions:\n",
    "Convolutions are foundational operations in image processing and particularly important in Convolutional Neural Networks (CNNs). They involve the process of sliding a smaller matrix (often called a kernel or filter) over a larger matrix to produce a new matrix (output). This process can be likened to sliding a flashlight over a large image to \"illuminate\" or \"focus\" on small portions at a time. As the flashlight moves over the image, it creates a new image representing what it \"sees\" or processes.\n",
    "\n",
    "### Mathematics of Convolution:\n",
    "At each position of the kernel over the input matrix, a local operation is performed. This local operation involves summing up the products of overlapping elements between the kernel and the portion of the larger matrix it covers.\n",
    "\n",
    "Visualize this process:\n",
    "Imagine you have a large matrix (the image) and a smaller matrix (the kernel). At each step, you align the kernel with a portion of the image, multiply the overlapping elements together, and then sum these products to produce a single number in the output matrix. This process is repeated for each position the kernel can occupy over the image.\n",
    "\n",
    "### 1D Convolutions:\n",
    "While we often use 2D convolutions for images (since images are 2D arrays of pixels), understanding 1D convolutions can simplify the concept.\n",
    "\n",
    "Consider two 1D arrays:\n",
    "- A larger array representing our \"image\"\n",
    "- A smaller array representing our \"kernel\"\n",
    "\n",
    "To compute the convolution, we will \"slide\" the kernel over the image, compute the product of overlapping elements, and sum these products.\n",
    "\n",
    "For example:\n",
    "Imagine a 1D \"image\" array [2, 1, 2, 1] and a \"kernel\" array [1, 0]. The convolution operation will involve sliding the kernel over the image and computing the dot product at each position.\n",
    "\n",
    "Let's manually compute this for the first position:\n",
    "\\[ (2 \\times 1) + (1 \\times 0) = 2 \\]\n",
    "\n",
    "The result for this position in the output array is 2.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2e311-e42d-4ed0-ba60-761dbb76383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our 1D image and kernel\n",
    "image_1D = np.array([2, 1, 2, 1])\n",
    "kernel_1D = np.array([1, 0])\n",
    "\n",
    "# Compute the convolution using numpy\n",
    "convolution_1D = np.convolve(image_1D, kernel_1D, mode='valid')\n",
    "convolution_1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0c6bd-3648-4dbc-a077-7375d0e47350",
   "metadata": {},
   "source": [
    "---\n",
    "The output array [2, 1, 2] represents the convolution of our image and kernel.\n",
    "\n",
    "### Hands-on Exercise for 1D Convolution:\n",
    "\n",
    "1. Given the 1D \"image\" array [3, 4, 2, 1] and \"kernel\" array [0, 1], compute the convolution manually for each position.\n",
    "2. Validate your result using numpy.\n",
    "3. Interpret the resulting convolution array. How does the choice of kernel values influence the output?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22c70d-79da-433e-8fb2-68ce5f7cb690",
   "metadata": {},
   "source": [
    "---\n",
    "## 2D Convolutions:\n",
    "\n",
    "2D convolutions are a direct extension of the 1D convolution process, but instead of working with 1D arrays, we work with 2D matrices. This is particularly important for image processing, as images are naturally 2D arrays of pixel values.\n",
    "\n",
    "### Introduction to 2D Convolutions:\n",
    "\n",
    "Imagine an actual image as a large grid of numbers, where each number corresponds to the pixel intensity at that point. Similarly, our kernel will be a smaller grid of numbers. The convolution process involves sliding this kernel over our image grid and at each position, summing up the products of overlapping elements.\n",
    "\n",
    "### Visual Representation:\n",
    "\n",
    "Let's use a simple example to visually represent this:\n",
    "\n",
    "Image Matrix:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "2 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "Kernel:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "2 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "In the first position, we would compute the sum as follows:\n",
    "$$ [ (1 \\times 0) + (2 \\times 1) + (0 \\times 2) + (1 \\times 1) = 3 ] $$\n",
    "\n",
    "This value, 3, would be the top-left value in our resulting convolution matrix.\n",
    "\n",
    "### Let's compute this 2D convolution using numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf2438-c4e0-41ff-8c2a-b0d8e0f09437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "# Define our 2D image and kernel\n",
    "image_2D = np.array([[1, 2, 3], [0, 1, 0], [2, 1, 0]])\n",
    "kernel_2D = np.array([[0, 1], [2, 1]])\n",
    "\n",
    "# Compute the 2D convolution using numpy\n",
    "convolution_2D = signal.convolve2d(image_2D, kernel_2D, mode='valid')\n",
    "convolution_2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd63ca-8e2b-4257-89e7-f44a3f476d1a",
   "metadata": {},
   "source": [
    "---\n",
    "The resulting matrix represents the convolution of our image and kernel. \n",
    "\n",
    "### Interpretation:\n",
    "This matrix provides a \"filtered\" representation of the original image. The kernel effectively acts as a feature detector, emphasizing certain features in the input image and de-emphasizing others. The specific nature of the emphasis depends on the values in the kernel.\n",
    "\n",
    "### Hands-on Exercise for 2D Convolution:\n",
    "\n",
    "1. Take the following 2D \"image\" matrix:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "4 & 3 & 2 & 1 \\\\\n",
    "2 & 1 & 0 & 3 \\\\\n",
    "3 & 2 & 1 & 4 \\\\\n",
    "0 & 0, 1 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "and the kernel:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "2. Compute the convolution manually for at least one position.\n",
    "3. Validate the entire convolution result using numpy.\n",
    "4. Discuss: What do you observe in the resulting convolution matrix? Can you infer the purpose of this kernel?\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
